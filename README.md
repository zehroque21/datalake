# DataLake AWS - Plataforma de Engenharia de Dados

ğŸŒ **[Acesse a pÃ¡gina do projeto](https://zehroque21.github.io/datalake/)** para uma visÃ£o completa da soluÃ§Ã£o.

---

Este repositÃ³rio contÃ©m uma soluÃ§Ã£o completa de datalake que combina as melhores prÃ¡ticas de engenharia de dados com infraestrutura como cÃ³digo (IaC). A plataforma oferece um ambiente robusto para processamento e anÃ¡lise de dados em escala, utilizando Apache Airflow para orquestraÃ§Ã£o e Delta Lake para armazenamento confiÃ¡vel.

## ğŸš€ VisÃ£o Geral

A soluÃ§Ã£o provisiona automaticamente uma infraestrutura moderna de datalake na Amazon Web Services (AWS), integrando:

- **Apache Airflow** para orquestraÃ§Ã£o de pipelines de dados
- **Delta Lake** para armazenamento transacional e versionado
- **AWS S3** para storage escalÃ¡vel e durÃ¡vel
- **EC2** otimizada para processamento de dados
- **CI/CD** integrado via GitHub Actions

## ğŸ—ï¸ Infraestrutura Provisionada

O Terraform provisiona os seguintes recursos na AWS:

### 1. InstÃ¢ncia EC2 (`aws_instance.airflow_vm`)
- **Tipo:** `t3.micro` (otimizada para cargas de trabalho de dados)
- **AMI:** Ubuntu Server 22.04 LTS (selecionada dinamicamente)
- **Finalidade:** Host para Apache Airflow, scripts Python, Spark e outras ferramentas de processamento

### 2. Bucket S3 (`data.aws_s3_bucket.datalake`)
- **Nome:** `datalake-bucket-for-airflow-and-delta-v2`
- **Finalidade:** Armazenamento principal do datalake para dados brutos, processados e tabelas Delta Lake

## âš™ï¸ AutomaÃ§Ã£o e Deploy

### GitHub Actions Workflow

O pipeline automatizado (`/.github/workflows/terraform.yaml`) executa:

1. **Limpeza de Recursos:** Remove instÃ¢ncias antigas para otimizar custos
2. **ValidaÃ§Ã£o:** Formata, valida e planeja mudanÃ§as no Terraform
3. **Provisionamento:** Aplica a infraestrutura na AWS
4. **Outputs:** Exibe informaÃ§Ãµes dos recursos criados

**Triggers:**
- Push para branch `main` (apenas arquivos em `/terraform/`)
- Pull Requests para `main`

### ConfiguraÃ§Ã£o de Credenciais

Configure os seguintes GitHub Secrets:
- `AWS_ACCESS_KEY_ID`: Chave de acesso AWS
- `AWS_SECRET_ACCESS_KEY`: Chave secreta AWS

## ğŸ“ Estrutura do Projeto

```
â”œâ”€â”€ terraform/                 # Infraestrutura como cÃ³digo
â”‚   â”œâ”€â”€ main.tf               # Recursos principais (EC2, S3)
â”‚   â”œâ”€â”€ variables.tf          # VariÃ¡veis do Terraform
â”‚   â””â”€â”€ outputs.tf            # Outputs dos recursos
â”œâ”€â”€ scripts/                  # Scripts de configuraÃ§Ã£o e exemplos
â”‚   â”œâ”€â”€ install_airflow.sh    # InstalaÃ§Ã£o do Apache Airflow
â”‚   â””â”€â”€ delta_lake_examples/  # Exemplos de uso do Delta Lake
â”‚       â”œâ”€â”€ write_delta_table.py
â”‚       â””â”€â”€ read_delta_table.py
â”œâ”€â”€ .github/workflows/        # Pipelines CI/CD
â”‚   â””â”€â”€ terraform.yaml        # Workflow principal
â””â”€â”€ index.html               # PÃ¡gina do projeto (GitHub Pages)
```

## ğŸ› ï¸ ConfiguraÃ§Ã£o e Uso

### 1. Deploy da Infraestrutura
```bash
# O deploy Ã© automÃ¡tico via GitHub Actions
# FaÃ§a push de alteraÃ§Ãµes em /terraform/ para disparar
git add terraform/
git commit -m "Update infrastructure"
git push origin main
```

### 2. Acesso Ã  InstÃ¢ncia EC2
```bash
# Conecte-se via SSH (configure sua chave SSH na AWS)
ssh -i sua-chave.pem ubuntu@<IP_PUBLICO_EC2>
```

### 3. InstalaÃ§Ã£o do Airflow
```bash
# Na instÃ¢ncia EC2, execute:
cd /path/to/repository
bash scripts/install_airflow.sh
```

### 4. Uso do Delta Lake
```python
# Exemplo de escrita
python scripts/delta_lake_examples/write_delta_table.py

# Exemplo de leitura
python scripts/delta_lake_examples/read_delta_table.py
```

## ğŸ”§ Tecnologias Utilizadas

- **Infraestrutura:** Terraform, AWS (EC2, S3, IAM)
- **OrquestraÃ§Ã£o:** Apache Airflow
- **Armazenamento:** Delta Lake, AWS S3
- **Processamento:** Python, Apache Spark
- **CI/CD:** GitHub Actions
- **Monitoramento:** Airflow Web UI

## ğŸ“Š Arquitetura da SoluÃ§Ã£o

```
GitHub Actions â†’ Terraform â†’ AWS EC2 (Airflow) â†’ AWS S3 (Delta Lake)
```

1. **IngestÃ£o:** Coleta de dados via pipelines Airflow
2. **Processamento:** TransformaÃ§Ãµes com Python/Spark
3. **Armazenamento:** Dados salvos em formato Delta Lake
4. **AnÃ¡lise:** Consultas e analytics sobre os dados processados

## ğŸ¯ Casos de Uso

- **ETL/ELT Pipelines:** Processamento automatizado de dados
- **Data Warehousing:** Armazenamento estruturado para analytics
- **Real-time Analytics:** Processamento de streams de dados
- **Machine Learning:** PreparaÃ§Ã£o de dados para modelos ML
- **Business Intelligence:** Dashboards e relatÃ³rios

## ğŸ‘¨â€ğŸ’» Sobre o Criador

**Amado Roque** - Engenheiro de Dados especializado em soluÃ§Ãµes de big data e analytics.

- ğŸ”— [LinkedIn](https://www.linkedin.com/in/amado-roque/)
- ğŸ™ [GitHub](https://github.com/zehroque21)

---

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

## ğŸ¤ ContribuiÃ§Ãµes

ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se Ã  vontade para abrir issues e pull requests.

---

**[ğŸŒ Visite a pÃ¡gina do projeto](https://zehroque21.github.io/datalake/)** para mais informaÃ§Ãµes e documentaÃ§Ã£o visual.

