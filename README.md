# DataLake AWS - Modern Data Engineering Platform

ğŸŒ **[Visit the project page](https://zehroque21.github.io/datalake/)** for a complete overview of the solution.

ğŸ”’ **[Security Policy](SECURITY.md)** - Read our comprehensive security guidelines.

---

This repository contains a **secure** and complete datalake solution that combines modern data engineering best practices with infrastructure as code (IaC). The platform provides a robust and hardened environment for data processing and analysis at scale, using **Prefect** for orchestration and Delta Lake for reliable storage.

## ğŸš€ Overview

The solution automatically provisions a modern, **security-hardened** datalake infrastructure on Amazon Web Services (AWS), integrating:

- **ğŸŒŠ Prefect** for modern data pipeline orchestration
- **ğŸ“Š Delta Lake** for transactional and versioned storage
- **â˜ï¸ AWS S3** for scalable and durable storage
- **ğŸ–¥ï¸ EC2** optimized and hardened for data processing
- **ğŸ”„ CI/CD** integrated via GitHub Actions
- **ğŸ”’ Security-first** approach with no SSH access

## ğŸŒŠ Why Prefect?

We chose **Prefect** over traditional orchestrators for its modern approach:

- âœ… **Python-first** - Native Python workflows, no complex DAG syntax
- âœ… **Modern UI** - Beautiful, intuitive web interface with real-time monitoring
- âœ… **Simple deployment** - No dependency hell, works out of the box
- âœ… **Better debugging** - Clear error messages and easy troubleshooting
- âœ… **Flexible execution** - Local development, cloud deployment, hybrid workflows
- âœ… **Type safety** - Built-in data validation and type checking

## ğŸ”’ Security Features

### Infrastructure Security
- âœ… **No SSH Access** - Instances accessible only via AWS Systems Manager
- âœ… **Encrypted Storage** - All EBS volumes encrypted at rest
- âœ… **Restrictive Security Groups** - Minimal network exposure
- âœ… **IAM Least Privilege** - Role-based access with minimal permissions
- âœ… **Private Networking** - No public IP addresses
- âœ… **Firewall Protection** - UFW enabled with secure defaults

### Application Security
- âœ… **Localhost Binding** - Prefect UI only accessible locally
- âœ… **Authentication Ready** - Configurable access controls
- âœ… **Secure Configuration** - Hardened settings
- âœ… **Service Isolation** - Dedicated user accounts
- âœ… **Automatic Updates** - Security patches applied automatically

## ğŸ—ï¸ Provisioned Infrastructure

Terraform provisions the following **secure** resources on AWS:

### 1. EC2 Instance (`aws_instance.prefect_vm`)
- **Type:** `t3.micro` (optimized for data workloads)
- **AMI:** Ubuntu Server 22.04 LTS (dynamically selected)
- **Security:** No SSH keys, encrypted storage, restrictive security group
- **Access:** AWS Systems Manager Session Manager only
- **Purpose:** Host for Prefect server, Python scripts, and data processing tools

### 2. Security Group (`aws_security_group.prefect_sg`)
- **Ingress:** Port 4200 for Prefect UI (configurable)
- **Egress:** HTTP/HTTPS for package installation, DNS resolution
- **Purpose:** Network-level security for EC2 instance

### 3. IAM Role and Policies
- **Role:** `prefect-ec2-role` with least privilege access
- **S3 Policy:** Access only to specific datalake bucket
- **SSM Policy:** Systems Manager access for secure connections
- **Purpose:** Secure credential management

### 4. S3 Bucket (`data.aws_s3_bucket.datalake`)
- **Name:** `datalake-bucket-for-prefect-and-delta-v2`
- **Access:** Restricted to EC2 instance IAM role only
- **Purpose:** Primary datalake storage for raw data, processed data, and Delta Lake tables

### 5. CloudWatch Log Group
- **Name:** `/aws/ec2/prefect`
- **Retention:** 30 days
- **Purpose:** Centralized logging and monitoring

## âš™ï¸ Automation and Deployment

### GitHub Actions Workflow

The automated pipeline (`/.github/workflows/terraform.yaml`) executes:

1. **Resource Cleanup:** Removes old instances to optimize costs
2. **Security Validation:** Validates Terraform security configurations
3. **Infrastructure Provisioning:** Applies secure infrastructure on AWS
4. **Security Outputs:** Displays secure access instructions

**Triggers:**
- Push to `main` branch (only files in `/terraform/`)
- Pull Requests to `main`

### Credentials Configuration

Configure the following GitHub Secrets with **least privilege** AWS credentials:
- `AWS_ACCESS_KEY_ID`: AWS access key (with EC2, S3, IAM permissions)
- `AWS_SECRET_ACCESS_KEY`: AWS secret key

## ğŸ“ Project Structure

```
â”œâ”€â”€ terraform/                 # Infrastructure as code (security-hardened)
â”‚   â”œâ”€â”€ main.tf               # Main resources with security configurations
â”‚   â”œâ”€â”€ variables.tf          # Terraform variables
â”‚   â””â”€â”€ outputs.tf            # Secure resource outputs
â”œâ”€â”€ flows/                    # ğŸŒŠ Prefect flows (shared dev/prod)
â”‚   â”œâ”€â”€ examples/             # Example data pipelines
â”‚   â”œâ”€â”€ etl/                  # ETL workflows
â”‚   â”œâ”€â”€ ml/                   # Machine learning pipelines
â”‚   â””â”€â”€ monitoring/           # Data quality and monitoring flows
â”œâ”€â”€ scripts/                  # Secure configuration scripts
â”‚   â”œâ”€â”€ install_prefect.sh    # Hardened Prefect installation
â”‚   â””â”€â”€ delta_lake_examples/  # Delta Lake usage examples
â”œâ”€â”€ docker/                   # ğŸ³ Local development environment
â”‚   â”œâ”€â”€ Dockerfile            # Prefect development container
â”‚   â”œâ”€â”€ docker-compose.yml    # Local Prefect stack
â”‚   â””â”€â”€ flows/               # â†’ Symlink to ../flows/
â”œâ”€â”€ .github/workflows/        # CI/CD pipelines
â”‚   â””â”€â”€ terraform.yaml        # Main workflow with security checks
â”œâ”€â”€ SECURITY.md              # Comprehensive security documentation
â”œâ”€â”€ index.html               # Project page (GitHub Pages)
â””â”€â”€ .gitignore              # Comprehensive ignore file for security
```

## ğŸ› ï¸ Development Workflow

### ğŸ  Local Development (Recommended)

1. **Start local Prefect environment:**
```bash
cd docker/
./test-prefect.sh
```

2. **Access local Prefect UI:**
```
http://localhost:4200
```

3. **Develop flows in `flows/` directory:**
```python
# flows/examples/my_pipeline.py
from prefect import flow, task

@task
def extract_data():
    return {"data": "example"}

@flow
def my_pipeline():
    data = extract_data()
    return data
```

4. **Test flows locally:**
```bash
cd flows/examples/
python my_pipeline.py
```

### â˜ï¸ Cloud Deployment

1. **Deploy infrastructure:**
```bash
# Automatic via GitHub Actions when pushing to main
git add flows/ terraform/
git commit -m "Add new pipeline"
git push origin main
```

2. **Access cloud Prefect:**
```bash
# Port forward from EC2 instance
aws ssm start-session \
    --target <INSTANCE_ID> \
    --document-name AWS-StartPortForwardingSession \
    --parameters '{"portNumber":["4200"],"localPortNumber":["4200"]}'
```

3. **Deploy flows to cloud:**
```bash
# Connect to instance
aws ssm start-session --target <INSTANCE_ID>

# Deploy flows
cd /home/prefect
source prefect-env/bin/activate
python flows/examples/my_pipeline.py
```

## ğŸš€ Quick Start

### 1. Infrastructure Deployment
```bash
# Clone repository
git clone https://github.com/zehroque21/datalake.git
cd datalake

# Deploy infrastructure (automatic via GitHub Actions)
git push origin main
```

### 2. Local Development Setup
```bash
# Start local Prefect environment
cd docker/
./test-prefect.sh

# Access UI at http://localhost:4200
```

### 3. Secure Cloud Access

**âš ï¸ IMPORTANT: No SSH access is available for security reasons.**

Use AWS Systems Manager Session Manager for secure access:

```bash
# Install and configure AWS CLI
aws configure

# Start secure session
aws ssm start-session --target <INSTANCE_ID>

# Port forward Prefect UI
aws ssm start-session \
    --target <INSTANCE_ID> \
    --document-name AWS-StartPortForwardingSession \
    --parameters '{"portNumber":["4200"],"localPortNumber":["4200"]}'
```

### 4. Delta Lake Usage
```python
# Write example
python scripts/delta_lake_examples/write_delta_table.py

# Read example
python scripts/delta_lake_examples/read_delta_table.py
```

## ğŸ”§ Technologies Used

- **Infrastructure:** Terraform, AWS (EC2, S3, IAM, Systems Manager)
- **Orchestration:** ğŸŒŠ Prefect (modern Python-first workflow engine)
- **Storage:** Delta Lake, AWS S3 (encrypted)
- **Processing:** Python, Apache Spark
- **Development:** Docker, Docker Compose
- **CI/CD:** GitHub Actions (with security validations)
- **Monitoring:** AWS CloudWatch, Prefect UI
- **Security:** AWS Systems Manager, IAM, Security Groups, EBS Encryption

## ğŸ“Š Solution Architecture

```
GitHub Actions â†’ Terraform â†’ AWS EC2 (Prefect) â†’ AWS S3 (Encrypted Delta Lake)
                     â†“
            AWS Systems Manager (Secure Access)
                     â†“
            Local Development (Docker + Prefect)
```

1. **ğŸ  Local Development:** Develop and test flows using Docker + Prefect
2. **â˜ï¸ Cloud Deployment:** Deploy flows to production Prefect instance
3. **ğŸ”’ Secure Processing:** Execute workflows in hardened AWS environment
4. **ğŸ“Š Encrypted Storage:** Store results in Delta Lake with encryption
5. **ğŸ“ˆ Monitoring:** Track execution via Prefect UI and CloudWatch

## ğŸ¯ Use Cases

- **ğŸ”„ Modern ETL/ELT Pipelines:** Python-first data processing workflows
- **ğŸ¢ Secure Data Warehousing:** Structured storage meeting compliance requirements
- **âš¡ Real-time Analytics:** Stream data processing with modern orchestration
- **ğŸ¤– Machine Learning Pipelines:** End-to-end ML workflows with data lineage
- **ğŸ“Š Business Intelligence:** Automated reporting and dashboard updates
- **ğŸ” Data Quality Monitoring:** Automated data validation and alerting

## ğŸŒŠ Prefect Flow Examples

### Basic ETL Pipeline
```python
from prefect import flow, task
import pandas as pd

@task
def extract_data(source: str):
    # Extract data from source
    return pd.read_csv(source)

@task
def transform_data(df: pd.DataFrame):
    # Transform data
    return df.dropna()

@task
def load_data(df: pd.DataFrame, destination: str):
    # Load to destination
    df.to_parquet(destination)

@flow
def etl_pipeline(source: str, destination: str):
    raw_data = extract_data(source)
    clean_data = transform_data(raw_data)
    load_data(clean_data, destination)
```

### Scheduled Data Pipeline
```python
from prefect import flow
from prefect.deployments import Deployment
from prefect.server.schemas.schedules import CronSchedule

@flow
def daily_report():
    # Your daily data processing logic
    pass

# Schedule to run daily at 6 AM
deployment = Deployment.build_from_flow(
    flow=daily_report,
    name="daily-report",
    schedule=CronSchedule(cron="0 6 * * *")
)
```

## ğŸ”’ Security Compliance

This infrastructure follows:
- **AWS Well-Architected Framework** - Security Pillar
- **NIST Cybersecurity Framework**
- **CIS Controls** for AWS
- **OWASP Top 10** for application security

## ğŸ‘¨â€ğŸ’» About the Creator

**Amado Roque** - Data Engineer specialized in modern data orchestration and secure analytics solutions.

- ğŸ”— [LinkedIn](https://www.linkedin.com/in/amado-roque/)
- ğŸ™ [GitHub](https://github.com/zehroque21)

---

## ğŸ“„ License

This project is under the MIT license. See the [LICENSE](LICENSE) file for more details.

## ğŸ¤ Contributions

Contributions are welcome! Please read our [Security Policy](SECURITY.md) before contributing.

## âš ï¸ Security Notice

This infrastructure is designed with security as the top priority. Please:
- Read the [Security Policy](SECURITY.md) before deployment
- Never commit secrets or credentials to the repository
- Use only the provided secure access methods
- Report security issues responsibly

---

**[ğŸŒ Visit the project page](https://zehroque21.github.io/datalake/)** for more information and visual documentation.

**[ğŸ”’ Read Security Policy](SECURITY.md)** for comprehensive security guidelines.

